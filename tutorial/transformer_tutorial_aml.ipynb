{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR0wGrIwGNR7"
      },
      "source": [
        "# Tutorial: Transformers, Vision Transformers and applications\n",
        "\n",
        "**Filled notebook:**\n",
        "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/metrics-lab/transformer-tutorial/blob/main/tutorial/transformers_tutorial.ipynb)\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/metrics-lab/transformer-tutorial/blob/main/tutorial/transformers_tutorial_aml.ipynb)  \n",
        "**Pre-trained models:**\n",
        "[![View files on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/github/metrics-lab/transformer-tutorial/blob/main/saved_models/)\n",
        "\n",
        "**Author:** Simon Dahan\n",
        "\n",
        "**Contact:** Please contact me at simon.dahan@kcl.ac.uk if you have questions on the tutorial or want to discuss some concepts further.\n",
        "\n",
        "\n",
        "In this tutorial, we will discuss one of the major deep learning architectures of the past years: the Transformers models - and uncover how it can be used in different applications for Natural Language Processing (NLP), Computer Vision (CV) and medical imaging. This tutorial is greatly inspired by the [*UVA Deep Learning Notebooks*](https://uvadlc-notebooks.readthedocs.io/en/latest/), particularly the [*Tutorial 6*](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.ipynb#scrollTo=1hkNROGHXvaz) and [*Tutorial 15*](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial15/Vision_Transformer.ipynb#scrollTo=DbUKvP9NXy-H), implemented with PyTorch Lightning. Here, we adapted the code in regular Pytorch and with some little twists to adapt to the case of medical imaging.  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK-wopQiAW6F"
      },
      "source": [
        "# Tutorial overview\n",
        "\n",
        "<details>\n",
        "    <summary><b>Part 1 - The Transformer Architecture (NLP)</b></summary>\n",
        "    <p><a href=\"#background\">1.1. Background</a></p>\n",
        "    <p><a href=\"#attention\">1.2 What is attention ? </a></p>\n",
        "    <p><a href=\"#attention_op\">1.3 Learning Keys, Queries and Weights</a></p>\n",
        "    <p><a href=\"#attention_op\">1.3 Scaled Dot Product Attention</a></p>\n",
        "    <p><a href=\"#multi head attention\">1.4. Multi-Head Attention</a></p>\n",
        "    <p><a href=\"#transformer encoder\">1.5. Transformer Encoder</a></p>\n",
        "    <p><a href=\"#positional encoding\">1.6. Positional Encoding</a></p>\n",
        "    <p><a href=\"#transformer model\">1.7. Transformer Model</a></p>\n",
        "</details>\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "    <summary><b>Part 2 - The Vision Transformer Architecture (CV)</b></summary>\n",
        "    <p><a href=\"#vision transformer\">2.1. Vision Transformers</a></p>\n",
        "    <p><a href=\"#experiments_part_2 transformer\">2.2. Experiments: Image classification</a></p>\n",
        "    <p><a href=\"#visualising attention\">2.3. Visualising Attention Maps</a></p>\n",
        "    \n",
        "</details>\n",
        "<br>\n",
        "<details>\n",
        "    <summary><b>Part 3 - Application 1: Vision Transformers in Medical Imaging</b></summary>\n",
        "    <p><a href=\"#background\">1.1. Background</a></p>\n",
        "</details>\n",
        "<br>\n",
        "<details>\n",
        "    <summary><b>Part 4 - Application 2: Surface Vision Transformers</b></summary>\n",
        "    <p><a href=\"#background\">1.1. Background</a></p>\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc9Xjp79AW6G"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Em9jO2iEcJFw",
        "outputId": "af0d584a-9a2a-4324-b771-d83920ca98cb"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import json\n",
        "from functools import partial\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"./data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/\"\n",
        "\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz-__qqALbD_"
      },
      "source": [
        "<a id=\"transformer\"></a>\n",
        "# 1. The Transformer Architecture (NLP)\n",
        "\n",
        "The Transformer architecture was introduced in [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al in 2017 in the context of machine translation and other Natural Language Processing (NLP) tasks. Transformer models have revolutionised the NLP field, surpassing the popular RNN and LSTM architectures, though proposing a *self-attention mechanism* which supports the modelling of long-range context. Nowadays, the Transformer architecture is the backbone of many popular Large Language Models such as *GPT* models [A. Radford et al 2019](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf) or *Mixtral* [A.Q. Jiang et al 2024](https://arxiv.org/pdf/2401.04088.pdf). More recently, they have shown potential as a domain agnostic architecure - allowing their adaption to natural image, graph and surface domains, as we will discuss later in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_JvABeLHHoo"
      },
      "source": [
        "<a id=\"#background\"></a>\n",
        "## 1.1. Some background and references\n",
        "\n",
        "\n",
        "In the first part of this notebook, we will implement the Transformer architecture by hand. As the architecture is so popular, there already exists a Pytorch module `nn.Transformer` ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)) and a [tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) on how to use it for next token prediction. However, we will implement part of it here ourselves, to get through to the smallest details.\n",
        "\n",
        "There are of course many more tutorials out there about attention and Transformers. Below, we list a few that are worth exploring if you are interested in the topic and might want yet another perspective on the topic after this one:\n",
        "\n",
        "* [Transformer: A Novel Neural Network Architecture for Language Understanding (Jakob Uszkoreit, 2017)](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) - The original Google blog post about the Transformer paper, focusing on the application in machine translation.\n",
        "* ⭐ [The Illustrated Transformer (Jay Alammar, 2018)](http://jalammar.github.io/illustrated-transformer/) - A very popular and great blog post intuitively explaining the Transformer architecture with many nice visualizations. The focus is on NLP.\n",
        "* ⭐ [Attention? Attention! (Lilian Weng, 2018)](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) - A nice blog post summarizing attention mechanisms in many domains including vision.\n",
        "* [Illustrated: Self-Attention (Raimi Karim, 2019)](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a) - A nice visualization of the steps of self-attention. Recommended going through if the explanation below is too abstract for you.\n",
        "* [The Transformer family (Lilian Weng, 2020)](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html) - A very detailed blog post reviewing more variants of Transformers besides the original one.\n",
        "\n",
        "<a id=\"#attention\"></a>\n",
        "## 1.2 What is Attention?\n",
        "\n",
        "The **self-attention mechanism** was introduced prior to the Transformer Architecture in various parallel works, notably in [*A structure self-attentive sentence embedding* Z.Lin et al 2017](https://arxiv.org/pdf/1703.03130.pdf). Therefore, there are a number of different definitions; however, the one we will use here is: '_the attention mechanism describes a weighted average of (sequence) elements, with weights dynamically computed based on an input query and elements' keys_.\n",
        "\n",
        " **So what does this exactly mean?** Let's take an example. Imagine that we are interested in understanding the meaning of an English sentence, the self-attention mechanism will  assess how much the meaning of each word depends on all others. For instance, here the self-attention mechanism will allow the model to *attend* to the words *The* and *Animal* in order to understand the meaning of the word *it*.\n",
        "\n",
        "<center width=\"100%\" style=\"padding:25px\"><img src=\"https://github.com/metrics-lab/transformer-tutorial/blob/main/tutorial/attention_example_1.png?raw=1\" height=\"600px\" width=\"600px\"></center>\n",
        "\n",
        "\n",
        "In practice, the goal is to update the value of each word by taking a weighted average of all other words in the sentence (or wider prose). This process is implemented dynamically, allowing the model to learn many different levels of meaning. This is achieved by breaking the self-attention mechanism down into four component parts:\n",
        "\n",
        "* **Query**: The query is a feature vector that describes the word/token that we are comparing against, i.e. what would we maybe want to pay attention to.\n",
        "* **Keys**: For each input element (word/token) we also have a key vector. This is what we compare the query against. The keys should be designed such that we can identify the words/tokens we want to pay attention to based on the query.\n",
        "* **Values**: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n",
        "* **Score function**: To rate which elements we want to pay attention to, we need to specify a score function $f_{attn}$. The score function takes the query and a key as input, and outputs an estimate of similarity for each query-key pair. It is usually implemented by a dot product (see section 1.3).\n",
        "\n",
        "The scores are then normalised and passed through a softmax to output self-attention weights. This will assign a higher contribution to value vectors whose key is most similar to the query. Visually, we can show the attention over a sequence of words as follows:\n",
        "\n",
        "<center width=\"100%\" style=\"padding:25px\"><img src=\"https://github.com/metrics-lab/transformer-tutorial/blob/main/tutorial/attention_example_2.svg?raw=1\" width=\"750px\"></center>\n",
        "\n",
        "For every word, we have one key and one value vector. The query is compared to all keys with a score function (in this case the dot product) to determine the weights. Finally, the value vectors of all words are averaged using the attention weights.\n",
        "\n",
        "<a id=\"#attention_basics\"></a>\n",
        "## 1.3 Learning Keys, Queries and Weights\n",
        "The keys, queries and weights are not given, rather they are learnt from the data by tuning weights matrices: $W_{1...h}^{Q}\\in\\mathbb{R}^{D \\times d_k}$, $W_{1...h}^{K}\\in\\mathbb{R}^{D \\times d_k}$, $W_{1...h}^{V}\\in\\mathbb{R}^{D \\times d_k}$ s.t for input $X\\in \\mathbb{R}^{T \\times D}$, $Q=XW^Q$, $K=XW^K$ and $V=XW^V$.\n",
        "\n",
        "Here $T$ is the sequence length (e.g. number of words in sentence, number of patches in an image), $D$ is the length of the input token, and $d_k$ and $d_v$ output length of queries/keys and values respectively. In practice, it is standard to keep token length the same throughout the network s.t $d_k=d_v=D$; going forward we will therefore stick to $D$ to represent all token/feature vector length throught the network.\n",
        "\n",
        "Note training learnable weights for $Q,K$ and $V$ is vital to allow the network to model different relationships beteen words/or tokens in order to learn a range of hierarchy of 'meanings' for each sequence (see slides).\n",
        "\n",
        "The final matrices therefore have shape: $Q\\in\\mathbb{R}^{T\\times D}$,$K\\in\\mathbb{R}^{T\\times D}$ and $V\\in\\mathbb{R}^{T\\times D}$.\n",
        "\n",
        "<a id=\"#attention_op\"></a>\n",
        "## 1.4 Scaled Dot Product Attention\n",
        "\n",
        "Our goal is to have an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute. This can be achieved through calculating a scaled dot product:\n",
        "\n",
        "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{D}}\\right)V$$\n",
        "\n",
        "Here matrix multiplication $QK^T$ performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape $T\\times T$. Each row then represents the attention logits for a specific query $i$ to keys representings all other elements in the sequence. On these, we normalise by the square root length of the key/query vectors ($\\sqrt D$) - in order to preserve unit variance  throughout the model - then apply a softmax. This provides self-attention weights which are then multipled with the values (for each element) to obtain a weighted mean. The computation graph for these operations is visualized below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/metrics-lab/transformer-tutorial/blob/main/tutorial/scaled_dot_product_attn.svg?raw=1\" width=\"210px\"></center>\n",
        "\n",
        "Note, the block `Mask (opt.)` in the diagram above represents the optional masking of specific entries in the attention matrix. We will not used it going forward.\n",
        "\n",
        "Let's start by writing a function which computes the output features given the triple of queries, keys, and values:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6LJDxVoAW6H"
      },
      "source": [
        "#### **Exercise 1** implementing the scaled dot product\n",
        "\n",
        "Here, you can find an implementation of the scaled dot product, essential for the attention computation.\n",
        "\n",
        "\n",
        "- `Task 1.1`: In the scale dot product function, apply the correct normalisation to the attention logits, before the softmax operation.\n",
        "\n",
        "**Hint**: check the attention operation above.\n",
        "\n",
        "- `Task 1.2`: In the \"Apply self-attention\" cell: create three torch random tensors for the Keys, Queries and Values. The sequence has a length of **3** and each token has a dimension of **6**.\n",
        "\n",
        "- `Task 1.3`: Answer the question and check the size of tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6_tHZBUAW6H"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------task 1 ------------------------------------------------------------\n",
        "# Task 1.1\n",
        "def scaled_dot_product(q, k, v):\n",
        "    #embedding size for normalisation\n",
        "    d_k = q.size()[-1]\n",
        "    #dot products between all query and keys pairs\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    #normalisation\n",
        "    attn_logits = attn_logits / None\n",
        "    #apply softmax to obtain attention score\n",
        "    attention = F.softmax(attn_logits, dim=-1)\n",
        "    #use the attention scores to weights the contribution of value tokens\n",
        "    #to genereate the new sequence\n",
        "    new_values = torch.matmul(attention, v)\n",
        "\n",
        "    return new_values, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9dF2LGNAW6I"
      },
      "outputs": [],
      "source": [
        "# Task 1.2\n",
        "\n",
        "seq_len = None\n",
        "emb_dim = None\n",
        "\n",
        "q = None\n",
        "k = None\n",
        "v = None\n",
        "\n",
        "values, attention = scaled_dot_product(q, k, v)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-uw-yNhAW6I"
      },
      "source": [
        "**Question: What do you expect the size of the Attention matrix to be? and why?**\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FV8mWWWMQhcP",
        "outputId": "93eaf6dc-5edb-434f-d4f1-8239414356e9"
      },
      "outputs": [],
      "source": [
        "print(\"Q has shape {}\\n\".format(q.shape), q)\n",
        "print('')\n",
        "print(\"K has shape {}\\n\".format(k.shape), k)\n",
        "print('')\n",
        "print(\"V has shape {}\\n\".format(v.shape), v)\n",
        "print('')\n",
        "print(\"Values have shape {}\\n\".format(values.shape), values)\n",
        "print('')\n",
        "print(\"Attention have shape {}\\n\".format(attention.shape), attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN7XDjrmctkT"
      },
      "source": [
        "<a id=\"multi head attention\"></a>\n",
        "## 1.5 Multi-Head Attention\n",
        "\n",
        "The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different levels of 'meaning' to any sequence, and this must be modelled through a range of self-attention weightings. This is addressed through the multi-head attention mechanism which models multiple different query-key-value triplets in parallel. Specifically, given a query, key, and value matrix, we transform those into $h$ sub-queries, sub-keys, and sub-values, which are then each passed independently through a scaled dot product attention operation. Afterward, the outputs of all heads are concatenated and combined with a final weight matrix $W^O$:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
        "    \\text{where } \\text{head}_i & = \\text{Attention}(Q_i,K_i, V_i)\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Here, Keys, Queries and Values are split across $h$ heads s.t. $Q_i\\in\\mathbb{R}^{T\\times D/h}$, $K_i\\in\\mathbb{R}^{T\\times D/h}$, $V_i\\in\\mathbb{R}^{T\\times D/h}$, and $W^{O}\\in\\mathbb{R}^{D \\times D}$ is the final projection matrix. Expressed in a computational graph, we can visualize it as (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/metrics-lab/transformer-tutorial/blob/main/tutorial/multihead_attention.svg?raw=1\" width=\"230px\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP4WstaXAW6I"
      },
      "source": [
        "#### **Exercise 2** implementing the Multi-Head Self-Attention Layer\n",
        "\n",
        "In practice, Keys Queries Values are computed together from a single linear layer (see lines 14 and 31 below). This means that we learn one large weights matrix with $3 \\times h \\times D$ rows - then split it into $h$ heads (row 34) then split these into the Query, Key, and Value for each head (line 36). This is done for computational efficiency.Finally these are pushed through the scaled dot product attention operation (line 40).   \n",
        "\n",
        "In this exercise we will take you through this process step by step.\n",
        "\n",
        "*First Complete the init function:*\n",
        "\n",
        "- `Task 2.1`: Complete the linear layer for estimation of the weights matrix (line 14).\n",
        "\n",
        "**Hint** For this you need to calculate what the number of output channels should be (number of rows of the output matrix). Remember, above we said it simultanesouly learns all weights for *all queries, keys and values, for all heads*.\n",
        "\n",
        "- `Task 2.2`: Reset the parameters of the layers (line 18)\n",
        "\n",
        "*Now complete the forward function:*\n",
        "\n",
        "- `Task 2.3`: Pass the correct tensors, in correct order, to the self-attention computation (line 40).\n",
        "\n",
        "**Hint** remember we defined this above. It calculates the self-attention weights and outputs the transformed values (```new_values```) and attention weights\n",
        "\n",
        "- `Task 2.4`: Implement the final linear layer (to multiply the output with $W^O$) (line 46, referencing line 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFQY_UP_AW6I"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Stack all weight matrices 1...h together for efficiency\n",
        "        # --------------------------------------------------task 2 ------------------------------------------------------------\n",
        "        # Task 2.1: implement keys, queries, values projection layer\n",
        "        self.qkv_proj = nn.Linear(input_dim, None)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Task 2.2: reset the parameters of the layers\n",
        "        None\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x,  return_attention=False):\n",
        "\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        qkv = self.qkv_proj(x)\n",
        "\n",
        "        # Separate Q, K, V from linear output\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # apply the attention layer\n",
        "        # Task 2.3: implement the scaled dot product attention\n",
        "        new_values, attention = scaled_dot_product(None, None, None)\n",
        "\n",
        "        new_values = new_values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "        new_values = new_values.reshape(batch_size, seq_length, self.embed_dim)\n",
        "\n",
        "        # Task 2.4: implement the final linear projection\n",
        "        out = self.out_proj(None)\n",
        "\n",
        "        # ---------------------------------------------------------------------------------------------------------------------s\n",
        "\n",
        "        if return_attention:\n",
        "            return out, attention\n",
        "        else:\n",
        "            return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDr1KCrsc1V_"
      },
      "source": [
        "One crucial characteristic of the multi-head attention is that it is permutation-equivariant with respect to its inputs. This means that if we switch two input elements in the sequence, e.g. $X_1\\leftrightarrow X_2$ (neglecting the batch dimension for now), the output is exactly the same besides the elements 1 and 2 switched. Hence, the multi-head attention is actually looking at the input not as a sequence, but as a set of elements. This property is what makes the multi-head attention block and the Transformer architecture so powerful! But what if the order of the input is actually important for solving the task? This is often a key component of the structure of languages *and images*. The answer is _Positional encodings _ which we will look at in sec 1.6 below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDsRg-I5c7-k"
      },
      "source": [
        "## 1.6 Transformer Encoder\n",
        "\n",
        "First lets look at how to apply the multi-head attention block inside the architecture of a Transformer encoder (see LHS of figure)\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/metrics-lab/transformer-tutorial/blob/main/tutorial/transformer_architecture.svg?raw=1\" width=\"400px\"></center>.\n",
        "\n",
        "credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "This applys $N$ identical layers/blocks, sequentially. Each layer implements a Multi-Head Attention block. The output is added to the input through a\n",
        "residual connection. Finally, the output is summed and passed through Layer Normalization.  \n",
        "\n",
        "The residual connection is crucial in the Transformer architecture for two reasons:\n",
        "\n",
        "1. Similar to ResNets, Transformers are designed to be very deep. Some models contain more than 24 blocks in the encoder. Hence, the residual connections are crucial for enabling a smooth gradient flow through the model, as they make it easier for the network to learn the identity operation when no transform is needed.\n",
        "2. Without the residual connection, the information about the original sequence is lost. Remember that the Multi-Head Attention layer ignores the position of elements in a sequence, and can only learn it from positional encoding. Removing the residual connections would mean that this information is lost.\n",
        "\n",
        "The Layer Normalization also plays a vital role as ensures all features are a similar magnitude within each toke, this provides regularisation, enabling faster training. We are not using Batch Normalization because batches are often small with Transformers as they require a lot of GPU memory. BatchNorm has anyway been shown to perform poorly for language since the features of word tokens display high variance (there are many, rare words that must be well modelled for a good distribution estimate).\n",
        "\n",
        "Finally, each layer contains a small fully-connected (feed-forward) network; this implements: Linear$\\to$ReLU$\\to$Linear MLP, reading each transformed token ($x$) separately. This means it treats each token identically:  \n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n",
        "    x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "This MLP adds extra complexity to the model, and can be seen as \"post-processing\" the output from the previous Multi-Head Attention operation, to prepare it for the next attention block. Usually, the inner dimensionality of the MLP is 2-8$\\times$ larger than $D$. The general advantage of a wider layer instead of a narrow, multi-layer MLP is the faster, parallelizable execution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9A9sILQAW6J"
      },
      "source": [
        "#### **Exercise 3** implementing the Encoder block\n",
        "\n",
        "Let's start implementing it with a single encoder block. Note the addition of dropout layers in the MLP and output of the MLP and Multi-Head Attention (for regularization).\n",
        "\n",
        "*Implement the following steps:*\n",
        "\n",
        "- `Task 3.1`: apply self attention layer on the input (line 33); remembering we have already defined this function above\n",
        "\n",
        "- `Task 3.2`: apply a dropout layer on the output of the attention; then add a residual layer (line 35 - one line!)\n",
        "\n",
        "- `Task 3.3`: apply the first Layer Normalisation layer (line 37)\n",
        "\n",
        "- `Task 3.4`: apply the feed forward network (MLP) on the output of the layer norm (line 41)\n",
        "\n",
        "- `Task 3.5`: again apply dropout and a residual layer (line 43)\n",
        "\n",
        "- `Task 3.6`: finally apply the second Layer Normalisation (line 45)\n",
        "\n",
        "**Hint** all of these operation are defined in the ```__init__``` function, and you just need to apply them!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptgAR22ic6eC"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim - Dimensionality of the input\n",
        "            num_heads - Number of heads to use in the attention block\n",
        "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
        "            dropout - Dropout probability to use in the dropout layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Attention layer\n",
        "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
        "\n",
        "        # Two-layer MLP\n",
        "        self.linear_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, input_dim)\n",
        "        )\n",
        "\n",
        "        # Layers to apply in between the main layers\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # --------------------------------------------------task 3 ------------------------------------------------------------\n",
        "        # Attention part\n",
        "        # Task 3.1 apply self attention layer on the input x\n",
        "        attn_out = None\n",
        "        # Task 3.2 apply dropout layer on the attention output and add a residual layer\n",
        "        x = None\n",
        "        # Task 3.3 apply normalisation layer\n",
        "        x = None\n",
        "\n",
        "        # MLP part\n",
        "        # Task 3.4 apply the feed forward network on the attention part output\n",
        "        linear_out = None\n",
        "        # Task 3.5 apply dropout layer on the FFN output and add a residual layer\n",
        "        x = None\n",
        "        # Task 3.6 apply the normalisation layer\n",
        "        x = None\n",
        "        # ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PARe8EIbdA0o"
      },
      "source": [
        "Based on this block, we can implement a module for the full Transformer encoder. Additionally to a forward function that iterates through the sequence of encoder blocks, we also provide a function called `get_attention_maps`. The idea of this function is to return the attention probabilities for all Multi-Head Attention blocks in the encoder. This helps us in understanding, and in a sense, explaining the model. However, the attention probabilities should be interpreted with some sceptism as they does not necessarily reflect the true interpretation of the model (there is a series of papers about this, including [Attention is not Explanation](https://arxiv.org/abs/1902.10186) and [Attention is not not Explanation](https://arxiv.org/abs/1908.04626))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlTnRt83cy0D"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, **block_args):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for l in self.layers:\n",
        "            x = l(x)\n",
        "        return x\n",
        "\n",
        "    def get_attention_maps(self, x):\n",
        "        attention_maps = []\n",
        "        for l in self.layers:\n",
        "            _, attn_map = l.self_attn(x, return_attention=True)\n",
        "            attention_maps.append(attn_map)\n",
        "            x = l(x)\n",
        "        return attention_maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfT8aRejcy2x"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            d_model - Hidden dimensionality of the input.\n",
        "            max_len - Maximum length of a sequence to expect.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
        "        # Used for tensors that need to be on the same device as the module.\n",
        "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
        "        self.register_buffer('pe', pe, persistent=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-LJ0jGadImT"
      },
      "source": [
        "## 1.7 Positional encoding\n",
        "<a id=\"positional encoding\"></a>\n",
        "\n",
        "We have discussed before that the Multi-Head Attention block is permutation-equivariant, and cannot distinguish whether an input comes before another one in the sequence or not; while highlighting *the order of tokens in a sequence is almost always important* for understanding context. To embed he concept of order into the network we must therefore add positional encodings. These may be learnt or pre-defined. In the original 'Attention is all you need paper' Vaswani et al. pre-defined embeddings from sine and cosine functions of different frequencies:\n",
        "\n",
        "$$\n",
        "PE_{(pos,i)} = \\begin{cases}\n",
        "    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n",
        "    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Here $PE_{(pos,i)}$ represents the position encoding at position $pos$ in the sequence, with hidden (feature) dimensionality $i$. These values, concatenated for all hidden dimensions, are summed to the original input features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynld_JPLdMhx"
      },
      "source": [
        "To understand the positional encoding, we can build from the following [PyTorch tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model). This will allow us to visualise the positional encoding, over feature dimensions and position in a sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "ar6o_lU5cy5h",
        "outputId": "d0e74c96-6e81-40e4-8517-1adabec996c6"
      },
      "outputs": [],
      "source": [
        "encod_block = PositionalEncoding(d_model=48, max_len=96)\n",
        "pe = encod_block.pe.squeeze().T.cpu().numpy()\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\n",
        "pos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\n",
        "fig.colorbar(pos, ax=ax)\n",
        "ax.set_xlabel(\"Position in sequence\")\n",
        "ax.set_ylabel(\"Hidden dimension\")\n",
        "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
        "ax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\n",
        "ax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "6JT_N1eDcy7j",
        "outputId": "cf719842-1120-4f31-f619-096c5c7855a6"
      },
      "outputs": [],
      "source": [
        "sns.set_theme()\n",
        "fig, ax = plt.subplots(2, 2, figsize=(12,4))\n",
        "ax = [a for a_list in ax for a in a_list]\n",
        "for i in range(len(ax)):\n",
        "    ax[i].plot(np.arange(1,17), pe[i,:16], color=f'C{i}', marker=\"o\", markersize=6, markeredgecolor=\"black\")\n",
        "    ax[i].set_title(f\"Encoding in hidden dimension {i+1}\")\n",
        "    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n",
        "    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n",
        "    ax[i].set_xticks(np.arange(1,17))\n",
        "    ax[i].tick_params(axis='both', which='major', labelsize=10)\n",
        "    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n",
        "    ax[i].set_ylim(-1.2, 1.2)\n",
        "fig.subplots_adjust(hspace=0.8)\n",
        "sns.reset_orig()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL5YgxCoHOEp"
      },
      "source": [
        "# 2. The Vision Transformer Architecture\n",
        "\n",
        "The motivation behind Transformers was to propose a mechanism that would support the learning of long-range attention to encode more complicated semantic concepts within language. The contribution of \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" [Alexey Dosovitskiy et al.](https://openreview.net/pdf?id=YicbFdNTTy) was to show that image-understanding also benefits from a more holistic view of each scene.\n",
        "\n",
        "Specifically, such Vision Transformers approach image understanding as a sequence modelling problem, by splitting images of, for example, $48\\times 48$ pixels into 9 $16\\times 16$ patches. In this sense, each patch is first embedded with a linear layer and then considered to be a \"word\"/\"token\". Positional encodings are then summed as before and a separate token is added for classification. Beyond this the sequence is treated like any other sequence model and processed with a series of Multi-Head Attention layers, with the exact same architecture as used for language models. A nice GIF visualization of the architecture is shown below (figure credit - [Phil Wang](https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif)):\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/metrics-lab/transformer-tutorial/blob/main/tutorial/vit.gif?raw=1\" width=\"600px\"></center>\n",
        "\n",
        "Let's start by importing everything we need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCQIUGM4omKa"
      },
      "outputs": [],
      "source": [
        "## Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "HoQTPN9rci5a",
        "outputId": "ebc64efe-ffe1-4c59-bf86-77bb0c6703d0"
      },
      "outputs": [],
      "source": [
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
        "                                     ])\n",
        "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
        "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
        "                                     ])\n",
        "# Loading the training dataset. We need to split it into a training and validation part\n",
        "# We need to do a little trick because the validation set should not use the augmentation.\n",
        "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
        "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
        "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
        "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
        "\n",
        "# Loading the test set\n",
        "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
        "\n",
        "# We define a set of data loaders that we can use for various purposes later.\n",
        "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
        "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
        "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
        "\n",
        "# Visualize some examples\n",
        "NUM_IMAGES = 16\n",
        "CIFAR_images = torch.stack([val_set[idx][0] for idx in range(NUM_IMAGES)], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(CIFAR_images, nrow=4, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"Image examples of the CIFAR10 dataset\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3_21PcwquCz"
      },
      "source": [
        "<a id=\"#vision transformers\"></a>\n",
        "## 2.1 Vision Transformers\n",
        "\n",
        "\n",
        "We will walk step by step through the Vision Transformer, and implement all parts by ourselves.\n",
        "\n",
        "The first step is to implement patching of each $N\\times N$ image into $(N/M)^2$ patches of size $M\\times M$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrHPcPG6ci8R"
      },
      "outputs": [],
      "source": [
        "def img_to_patch(x, patch_size, flatten_channels=True):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
        "        patch_size - Number of pixels per dimension of the patches (integer)\n",
        "        flatten_channels - If True, the patches will be returned in a flattened format\n",
        "                           as a feature vector instead of a image grid.\n",
        "    \"\"\"\n",
        "    B, C, H, W = x.shape\n",
        "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
        "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
        "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
        "    if flatten_channels:\n",
        "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSoa9DSKWQZx"
      },
      "source": [
        "#### **Exercise 6** Image patches\n",
        "\n",
        "Let's take a look at how that works for our $32\\times 32$ `CIFAR images` examples above.\n",
        "\n",
        "- `Task 6.1`: Visualise patches of different sizes (for instance 4, 8 or 16). What is the lenght of the sequence in each case?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCODDQbKWpEE"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------task 6 ------------------------------------------------------------\n",
        "# Task 6.1 Try out different patch sizes and visualize the patches\n",
        "img_patches = img_to_patch(CIFAR_images, patch_size=None, flatten_channels=False)\n",
        "\n",
        "fig, ax = plt.subplots(CIFAR_images.shape[0], 1, figsize=(14,3))\n",
        "fig.suptitle(\"Images as input sequences of patches\")\n",
        "for i in range(CIFAR_images.shape[0]):\n",
        "    img_grid = torchvision.utils.make_grid(img_patches[i], nrow=64, normalize=True, pad_value=0.9)\n",
        "    img_grid = img_grid.permute(1, 2, 0)\n",
        "    ax[i].imshow(img_grid)\n",
        "    ax[i].axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0s5q-sMrEMZ"
      },
      "source": [
        "Compared to the original images, it is much harder to recognize the objects from those patch lists now. The inductive bias in CNNs that an image is a grid of pixels, is lost in this input format. With the help of positional encodings, the model must to learn for itself how to combine the patches to recognize the objects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-B3NeehrMaA"
      },
      "source": [
        "<a id=\"#experiments_part_2\"></a>\n",
        "## 2.2 Experiments: Image Classification\n",
        "\n",
        "Let's now try out the Vision Transformer (ViT) on image classification.\n",
        "We will use the github repository [ViT Pytorch](https://github.com/lucidrains/vit-pytorch) which implements many of the latest vision transformer architectures. We will compare the performance of the ViT against a ResNet CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHnoIy2jsoQl",
        "outputId": "784e7591-d659-41c5-e60b-1908ac3d19c7"
      },
      "outputs": [],
      "source": [
        "#install library\n",
        "!pip install vit-pytorch timm\n",
        "\n",
        "import torch\n",
        "from vit_pytorch import ViT\n",
        "import timm\n",
        "from torch.optim.lr_scheduler import StepLR  # Or any other scheduler you prefer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97OJmOoCYhTo"
      },
      "source": [
        "#### **Exercise 7** Comparing Vision Transformer and CNN for image classification\n",
        "\n",
        "Let's first  see if vision transformers can indeed solve image classification tasks.\n",
        "\n",
        "- `Task 7.1`: Instantiate a ViT model from the vit_pytorch library. `patch_size=4` is set to 4 by default but feel free to try different values to see how it impacts the training models.\n",
        "\n",
        "- `Task 7.2`: Set the training parameters to `n_epochs=10` and `lr=0.001`. Then, try to improve the accuracy by playing with the parameters. You can also modify the scheduler.\n",
        "\n",
        "- `Task 7.3`: Train the vision transformer model and report the accuracy after 10 epochs.\n",
        "\n",
        "- `Task 7.4`: Instantiate a `resnet50` model using the timm library and compare the validation accuracy between CNN and ViT.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKM2cIIyci-X",
        "outputId": "e52f78dc-4fa9-4b65-994f-9d58cb3de7d9"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------task 7 ------------------------------------------------------------\n",
        "# Task 7.1: instantiate the model here\n",
        "transformer_model = ViT(\n",
        "                          image_size = None, #height/width of the image\n",
        "                          patch_size = 4, #size of the patch to form the sequence\n",
        "                          num_classes = None,\n",
        "                          dim = 192, #embedding dimension D\n",
        "                          depth = 12, #number of encoder layers L\n",
        "                          heads = 3, # number of heads in the MHSA\n",
        "                          mlp_dim = 4*192, #embedding dimension of the FFN\n",
        "                          dropout = 0.0, #dropout prior to the first transformer layer\n",
        "                          emb_dropout = 0.0 #dropout applied between the MHSA and FFN\n",
        "                      )\n",
        "\n",
        "\n",
        "transformer_model.to(device)\n",
        "\n",
        "print('Number of parameters ViT model: {:,}'.format(sum(p.numel() for p in transformer_model.parameters() if p.requires_grad)))\n",
        "\n",
        "# Task 7.2: Play with the hyperparameters\n",
        "n_epochs = None\n",
        "optimizer = optim.Adam(transformer_model.parameters(), lr=None)  # Set your own learning rate\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)  # Adjust according to your needs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brKb6-_nZOLe"
      },
      "outputs": [],
      "source": [
        "# Task 7.3: Train the model\n",
        "for epoch in range(n_epochs):  # Set your own number of epochs\n",
        "    transformer_model.train()\n",
        "    for batch in train_loader:\n",
        "        inputs, targets = batch\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = transformer_model(inputs)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()  # Adjust learning rate\n",
        "\n",
        "    # Validation step\n",
        "    transformer_model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = transformer_model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "        val_accuracy = correct / total\n",
        "        print(f'Epoch {epoch}, Val Accuracy: {val_accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgF40pZDaCsS"
      },
      "outputs": [],
      "source": [
        "# Task 7.4: Train a ResNet Model\n",
        "resnet_model = timm.create_model(None, pretrained=False, num_classes=10)\n",
        "resnet_model.to(device)\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR  # Or any other scheduler you prefer\n",
        "\n",
        "n_epochs = 10\n",
        "optimizer = optim.Adam(resnet_model.parameters(), lr=0.001)  # Set your own learning rate\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)  # Adjust according to your needs\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):  # Set your own number of epochs\n",
        "    resnet_model.train()\n",
        "    for batch in train_loader:\n",
        "        inputs, targets = batch\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resnet_model(inputs)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()  # Adjust learning rate\n",
        "\n",
        "    # Validation step\n",
        "    resnet_model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = resnet_model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "        val_accuracy = correct / total\n",
        "        print(f'Epoch {epoch}, Val Accuracy: {val_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6f4Dzmfeelm"
      },
      "source": [
        "**Question: Which model between the CNN and the ViT give the best classification results? Why do you think it is the case?**\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aJKYu8fAW6Q"
      },
      "source": [
        "<a id=\"#visualising attention\"></a>\n",
        "## 2.3 Visualising attention maps\n",
        "\n",
        "Over the years, various interactive tools have been developed to visualise the attention weights learn while training transformers on images or text. Here are a few examples that you can explore:\n",
        "\n",
        "- [AttentionViz: A Global View of Transformer Attention](https://catherinesyeh.github.io/attn-docs/#view-info)\n",
        "- ⭐ [Visualization of Self-Attention Maps in Vision](https://epfml.github.io/attention-cnn/) (my personal favourite tool)\n",
        "\n",
        "\n",
        "<center width=\"100%\" style=\"padding:25px\"><img src=\"attention_maps.png\" height=\"500px\" width=\"800px\"></center>\n",
        "\n",
        "\n",
        "You will notice that some of the attention maps looks a bit like segmentation maps. This idea has inspired various models to achieve high segmentation accuracy without any groudtruth labels but only via self-supervision. One of the most notable model is [DINO: Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294) by M.Caron et al 2021. See results below:\n",
        "\n",
        "<center width=\"100%\" style=\"padding:25px\"><img src=\"attention_maps2.png\" height=\"200px\" width=\"800px\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7i8-SR4HSwv"
      },
      "source": [
        "# 3. Vision Transformers in Medical Imaging\n",
        "\n",
        "The Vision Transformer models have naturally been adapted and tested on medical dataset. They present a lot of potential as they seem to be able to inherently model long-range spatial dependencies. This is very valuable in medical imaging settings where, for instance tumours can be scattered over the entire scan or organs can have very particuler shape.\n",
        " This [survey paper](https://arxiv.org/pdf/2201.09873.pdf) reviews many deep learning papers that adapt vision transformers in the medical field, for task such assegmentation, classification, registration or anomaly detection. This Github repository [Awesome Medical Transformer](https://github.com/fahadshamshad/awesome-transformers-in-medical-imaging) also compiles the open-source codebase available of the latest transformer medical publications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHZGIWnlAW6Q",
        "outputId": "71d2ffbe-9407-4221-8d59-a4ff8ce5e2dc"
      },
      "outputs": [],
      "source": [
        "!pip install monai\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from monai.apps import download_and_extract\n",
        "from monai.data import decollate_batch, DataLoader\n",
        "from monai.metrics import ROCAUCMetric\n",
        "from monai.transforms import (\n",
        "    Activations,\n",
        "    EnsureChannelFirst,\n",
        "    AsDiscrete,\n",
        "    Compose,\n",
        "    LoadImage,\n",
        "    RandFlip,\n",
        "    RandRotate,\n",
        "    RandZoom,\n",
        "    ScaleIntensity,\n",
        ")\n",
        "\n",
        "\n",
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = './data/'\n",
        "\n",
        "resource = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz\"\n",
        "md5 = \"0bc7306e7427e00ad1c5526a6677552d\"\n",
        "\n",
        "compressed_file = os.path.join(root_dir, \"MedNIST.tar.gz\")\n",
        "data_dir = os.path.join(root_dir, \"MedNIST\")\n",
        "if not os.path.exists(data_dir):\n",
        "    download_and_extract(resource, compressed_file, root_dir, md5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhFxD23hvpa5"
      },
      "source": [
        "## 3.1 MedNIST dataset\n",
        " Here we will use the popular **Monai** library to implement a image classification on the MedNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcYVdX-TAW6Q",
        "outputId": "6e2b2f9a-f550-4331-f36b-2798151e12bc"
      },
      "outputs": [],
      "source": [
        "class_names = sorted(x for x in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, x)))\n",
        "num_class = len(class_names)\n",
        "image_files = [\n",
        "    [os.path.join(data_dir, class_names[i], x) for x in os.listdir(os.path.join(data_dir, class_names[i]))]\n",
        "    for i in range(num_class)\n",
        "]\n",
        "num_each = [len(image_files[i]) for i in range(num_class)]\n",
        "image_files_list = []\n",
        "image_class = []\n",
        "for i in range(num_class):\n",
        "    image_files_list.extend(image_files[i])\n",
        "    image_class.extend([i] * num_each[i])\n",
        "num_total = len(image_class)\n",
        "image_width, image_height = PIL.Image.open(image_files_list[0]).size\n",
        "\n",
        "print(f\"Total image count: {num_total}\")\n",
        "print(f\"Image dimensions: {image_width} x {image_height}\")\n",
        "print(f\"Label names: {class_names}\")\n",
        "print(f\"Label counts: {num_each}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "0WhT0ZwSAW6R",
        "outputId": "fefe5a67-2696-4cc6-8533-fe3efb40aa85"
      },
      "outputs": [],
      "source": [
        "plt.subplots(3, 3, figsize=(8, 8))\n",
        "for i, k in enumerate(np.random.randint(num_total, size=9)):\n",
        "    im = PIL.Image.open(image_files_list[k])\n",
        "    arr = np.array(im)\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.xlabel(class_names[image_class[k]])\n",
        "    plt.imshow(arr, cmap=\"gray\", vmin=0, vmax=255)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjeD-_tYAW6R",
        "outputId": "e9f9edf8-a56e-41b3-83c8-250d90f232d0"
      },
      "outputs": [],
      "source": [
        "val_frac = 0.1\n",
        "test_frac = 0.1\n",
        "length = len(image_files_list)\n",
        "indices = np.arange(length)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "test_split = int(test_frac * length)\n",
        "val_split = int(val_frac * length) + test_split\n",
        "test_indices = indices[:test_split]\n",
        "val_indices = indices[test_split:val_split]\n",
        "train_indices = indices[val_split:]\n",
        "\n",
        "train_x = [image_files_list[i] for i in train_indices]\n",
        "train_y = [image_class[i] for i in train_indices]\n",
        "val_x = [image_files_list[i] for i in val_indices]\n",
        "val_y = [image_class[i] for i in val_indices]\n",
        "test_x = [image_files_list[i] for i in test_indices]\n",
        "test_y = [image_class[i] for i in test_indices]\n",
        "\n",
        "print(f\"Training count: {len(train_x)}, Validation count: \" f\"{len(val_x)}, Test count: {len(test_x)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b01kNEi-AW6R"
      },
      "outputs": [],
      "source": [
        "train_transforms = Compose(\n",
        "    [\n",
        "        LoadImage(image_only=True),\n",
        "        EnsureChannelFirst(),\n",
        "        ScaleIntensity(),\n",
        "        RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),\n",
        "        RandFlip(spatial_axis=0, prob=0.5),\n",
        "        RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = Compose([LoadImage(image_only=True), EnsureChannelFirst(), ScaleIntensity()])\n",
        "\n",
        "y_pred_trans = Compose([Activations(softmax=True)])\n",
        "y_trans = Compose([AsDiscrete(to_onehot=num_class)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laqn-zakAW6R",
        "outputId": "45f015e1-b11c-4d01-c237-94a144db265c"
      },
      "outputs": [],
      "source": [
        "class MedNISTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_files, labels, transforms):\n",
        "        self.image_files = image_files\n",
        "        self.labels = labels\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.transforms(self.image_files[index]), self.labels[index]\n",
        "\n",
        "\n",
        "train_ds = MedNISTDataset(train_x, train_y, train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=300, shuffle=True, num_workers=10)\n",
        "\n",
        "val_ds = MedNISTDataset(val_x, val_y, val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=300, num_workers=10)\n",
        "\n",
        "test_ds = MedNISTDataset(test_x, test_y, val_transforms)\n",
        "test_loader = DataLoader(test_ds, batch_size=300, num_workers=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe6OJoRMtSZg"
      },
      "source": [
        "#### **Exercise 8** Define ViT model and train\n",
        "\n",
        "**Monai** has its own implementations of deep learning models such as Vision Transformers, to accommodate for medical images format and particularities.\n",
        "\n",
        "- `Task 8.1`: Define the ViT model from the monai library with the correct parameters `in_channels`, `img_size`, `patch_size`\n",
        "\n",
        "- `Task 8.2`: Train the ViT model model (it will take a couple of minutes)\n",
        "\n",
        "- `Task 8.3`: Load the best model and evaluate its performance on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52jP-zliAW6R"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------task 8 ------------------------------------------------------------\n",
        "\n",
        "from monai.networks.nets import ViT\n",
        "\n",
        "# Task 8.1: instantiate the model here\n",
        "model = ViT(in_channels=None,\n",
        "            img_size=None,\n",
        "            patch_size=None,\n",
        "            hidden_size=192,\n",
        "            mlp_dim=192*4,\n",
        "            num_layers=12,\n",
        "            num_heads=3,\n",
        "            pos_embed='conv',\n",
        "            proj_type='conv',\n",
        "            pos_embed_type='learnable',\n",
        "            classification=True,\n",
        "            num_classes=num_class,\n",
        "            dropout_rate=0.0,\n",
        "            spatial_dims=2,\n",
        "            post_activation=False,\n",
        "            qkv_bias=False,\n",
        "            save_attn=False).to(device)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n",
        "max_epochs = 4\n",
        "val_interval = 1\n",
        "auc_metric = ROCAUCMetric()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgENfpXBAW6R"
      },
      "source": [
        "## 3.2 Training vision transformer model on medical images\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBr9qA7tAW6R",
        "outputId": "1112aec8-ec7a-415a-91b8-7e6deb5cad15"
      },
      "outputs": [],
      "source": [
        "# Task 8.2: train the model\n",
        "\n",
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "writer = SummaryWriter()\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    for batch_data in train_loader:\n",
        "        step += 1\n",
        "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs[0], labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        print(f\"{step}/{len(train_ds) // train_loader.batch_size}, \" f\"train_loss: {loss.item():.4f}\")\n",
        "        epoch_len = len(train_ds) // train_loader.batch_size\n",
        "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    if (epoch + 1) % val_interval == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
        "            y = torch.tensor([], dtype=torch.long, device=device)\n",
        "            for val_data in val_loader:\n",
        "                val_images, val_labels = (\n",
        "                    val_data[0].to(device),\n",
        "                    val_data[1].to(device),\n",
        "                )\n",
        "                y_pred = torch.cat([y_pred, model(val_images)[0]], dim=0)\n",
        "                y = torch.cat([y, val_labels], dim=0)\n",
        "            y_onehot = [y_trans(i) for i in decollate_batch(y, detach=False)]\n",
        "            y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\n",
        "            auc_metric(y_pred_act, y_onehot)\n",
        "            result = auc_metric.aggregate()\n",
        "            auc_metric.reset()\n",
        "            del y_pred_act, y_onehot\n",
        "            metric_values.append(result)\n",
        "            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
        "            acc_metric = acc_value.sum().item() / len(acc_value)\n",
        "            if result > best_metric:\n",
        "                best_metric = result\n",
        "                best_metric_epoch = epoch + 1\n",
        "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model.pth\"))\n",
        "                print(\"saved new best metric model\")\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current AUC: {result:.4f}\"\n",
        "                f\" current accuracy: {acc_metric:.4f}\"\n",
        "                f\" best AUC: {best_metric:.4f}\"\n",
        "                f\" at epoch: {best_metric_epoch}\"\n",
        "            )\n",
        "            writer.add_scalar(\"val_accuracy\", acc_metric, epoch + 1)\n",
        "\n",
        "print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "qp9UAMGaAW6R",
        "outputId": "2fa577f8-31c0-4c32-8559-3be365375ec7"
      },
      "outputs": [],
      "source": [
        "plt.figure(\"train\", (12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"Epoch Average Loss\")\n",
        "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
        "y = epoch_loss_values\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"Val AUC\")\n",
        "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
        "y = metric_values\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rd8HYa2AW6R"
      },
      "source": [
        "### Evaluate the model on test dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Rd1Mt_9AW6R",
        "outputId": "a4e6ecfb-8abd-436a-9af6-d7e9b51c4a6a"
      },
      "outputs": [],
      "source": [
        "# Task 8.3: evaluate the model performances\n",
        "model.load_state_dict(torch.load(None))\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for test_data in test_loader:\n",
        "        test_images, test_labels = (\n",
        "            test_data[0].to(device),\n",
        "            test_data[1].to(device),\n",
        "        )\n",
        "        pred = model(test_images)[0].argmax(dim=1)\n",
        "        for i in range(len(pred)):\n",
        "            y_true.append(test_labels[i].item())\n",
        "            y_pred.append(pred[i].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN84q91hAW6R",
        "outputId": "61e63b7d-a919-452b-896b-7b5019d23d81"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na1xWVDOHW_z"
      },
      "source": [
        "# 4. Application: Surface Vision Transformers\n",
        "\n",
        "\n",
        "In this last part of the tutorial, we will investigate extension of the Vision Transformer to non-Euclidean geometries. As you probably already understood, the transformer achitecture can be used in many different data domains. We often describe the transformer architecture as *agnostic* to the domain, as long as the input data can be represented as a sequence of tokens. [Dahan et al 2022](https://arxiv.org/abs/2203.16414) extented the vision transformer architecture to study cortical surfaces represented on regular icosehadron. This is achivied by patching sphericalised meshes using low resolution icospheral grids and then using a regular ViT to process the cortical patches. The model is named Surface Vision Trasnformer (SiT).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/metrics-lab/transformer-tutorial/blob/main/tutorial/sit_gif.gif?raw=1\"  width=\"800px\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtEdkpRemGQr"
      },
      "source": [
        "First let's clone the code for the SiT model and install some necessary dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5NrBZyAmFG9",
        "outputId": "6a29f8b0-2e3b-4a32-af8d-76801cf92a42"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/metrics-lab/surface-vision-transformers.git\n",
        "!pip install einops\n",
        "!pip install vit-pytorch timm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('./surface-vision-transformers')\n",
        "from models.sit import SiT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTzpXrHWAW6R"
      },
      "source": [
        "## 4.1 dHCP dataset\n",
        "\n",
        "Here you will be downloading data from the dHCP dataset. The data has already been processed in order to use the SiT out-of-the-box. Four cortical metrics (myelin maps, cortical thickness, sulcal depth, curvature) are used and ico6 sphericalised meshes are patched using a ico2 sphericalised grid. This leads to a sequence of 320 non-overlapping patching of 153 vertices each (see illustration).\n",
        "\n",
        "We will use data from the scan age experiment as per [A. Fawaz et al 2021](https://www.biorxiv.org/content/10.1101/2021.12.01.470730v1.full.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntxgbMfFlyWe",
        "outputId": "0908b9f7-ee3e-4b14-e94d-c58241f5d4fc"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/uc?id=1DJhrERb1hk8Ekp_Cq2qwnxYvjq2nM0kc\n",
        "\n",
        "!unzip -q dhcp_scan_age_template_processed.zip -d ./data/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtoY9Y5ul1Jn",
        "outputId": "429eb53d-b989-4bc7-d69d-cb54e35e28ca"
      },
      "outputs": [],
      "source": [
        "train_data = np.load('./data/train_data.npy')\n",
        "validation_data = np.load('./data/validation_data.npy')\n",
        "\n",
        "train_labels = np.load('./data/train_labels.npy')\n",
        "validation_labels = np.load('./data/validation_labels.npy')\n",
        "\n",
        "print(train_data.shape, validation_data.shape, train_labels.shape, validation_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfvD0QATwwum"
      },
      "source": [
        "**Question: To what correspond each dimension of the train_data tensor?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "- 1st dimension:  ?\n",
        "- 2nd dimension:  ?\n",
        "- 3rd dimension:  ?\n",
        "- 4th dimension:  ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eYV6029mZ1K",
        "outputId": "1af3d9dc-95eb-49e6-cda7-fc2177318722"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "\n",
        "train_data_dataset = torch.utils.data.TensorDataset(torch.from_numpy(train_data).float(),\n",
        "                                                    torch.from_numpy(train_labels).float())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data_dataset,\n",
        "                                                batch_size = batch_size,\n",
        "                                                shuffle=True,\n",
        "                                                num_workers=16)\n",
        "\n",
        "val_data_dataset = torch.utils.data.TensorDataset(torch.from_numpy(validation_data).float(),\n",
        "                                                torch.from_numpy(validation_labels).float())\n",
        "\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_data_dataset,\n",
        "                                        batch_size = batch_size,\n",
        "                                        shuffle=False,\n",
        "                                        num_workers=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDy2QKh3AW6R"
      },
      "source": [
        "## 4.2 SiT training\n",
        "\n",
        "We will train the model for task of scan age (PMA) prediction. This is a regressiont task. Therefore, we will use the MSELoss to evaluate the model and the Adam optimiser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Exercise 9** Define the SiT model and train on PMA prediction\n",
        "\n",
        "- `Task 9.1`: Using the dimension values from the previous questions, defined the SiT model correctly\n",
        "\n",
        "- `Task 9.2`: Train the model and comment on its performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgfNCRMDmWj9"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------task 9 ------------------------------------------------------------\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Task 9.1: instantiate the model here\n",
        "sit_model = SiT(dim=192,\n",
        "            depth=12,\n",
        "            heads=3,\n",
        "            mlp_dim=4*192,\n",
        "            pool='cls',\n",
        "            num_patches=None,\n",
        "            num_classes=1,\n",
        "            num_channels=None,\n",
        "            num_vertices=None,\n",
        "            dim_head=64,\n",
        "            dropout=0.0,\n",
        "            emb_dropout=0.0)\n",
        "\n",
        "sit_model.to(device)\n",
        "\n",
        "num_training_epochs=100\n",
        "num_val_epochs=10\n",
        "criterion = nn.MSELoss(reduction='mean')\n",
        "optimizer = optim.Adam(sit_model.parameters(), lr=0.0003, weight_decay=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "N7qTXxJ3mWmc",
        "outputId": "563e8e6a-2df5-41d4-cd90-8e463e9b5846"
      },
      "outputs": [],
      "source": [
        "# Task 9.2: Train the model here\n",
        "best_mae = 100000000\n",
        "mae_val_epoch = 100000000\n",
        "running_val_loss = 100000000\n",
        "\n",
        "for epoch in range(num_training_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "\n",
        "    sit_model.train()\n",
        "\n",
        "    targets_ =  []\n",
        "    preds_ = []\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "\n",
        "        inputs, targets = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = sit_model(inputs)\n",
        "\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        targets_.append(targets.cpu().numpy())\n",
        "        preds_.append(outputs.reshape(-1).cpu().detach().numpy())\n",
        "\n",
        "    mae_epoch = np.mean(np.abs(np.concatenate(targets_) - np.concatenate(preds_)))\n",
        "\n",
        "    if (epoch+1)%5==0:\n",
        "        print('| Epoch - {} | Loss - {:.4f} | MAE - {:.4f} | LR - {}'.format(epoch+1, running_loss/(i+1), round(mae_epoch,4), optimizer.param_groups[0]['lr']))\n",
        "\n",
        "    ##############################\n",
        "    ######    VALIDATION    ######\n",
        "    ##############################\n",
        "\n",
        "    if (epoch+1)%num_val_epochs==0:\n",
        "\n",
        "        running_val_loss = 0\n",
        "\n",
        "        sit_model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            targets_ = []\n",
        "            preds_ = []\n",
        "\n",
        "            for i, data in enumerate(val_loader):\n",
        "\n",
        "                inputs, targets = data[0].to(device), data[1].to(device)\n",
        "\n",
        "                outputs = sit_model(inputs)\n",
        "\n",
        "                loss = criterion(outputs.squeeze(), targets)\n",
        "\n",
        "                running_val_loss += loss.item()\n",
        "\n",
        "                targets_.append(targets.cpu().numpy())\n",
        "                preds_.append(outputs.reshape(-1).cpu().numpy())\n",
        "\n",
        "\n",
        "        mae_val_epoch = np.mean(np.abs(np.concatenate(targets_)- np.concatenate(preds_)))\n",
        "\n",
        "        print('| Validation | Epoch - {} | Loss - {:.4f} | MAE - {:.4f} |'.format(epoch+1, running_val_loss, mae_val_epoch ))\n",
        "\n",
        "        if mae_val_epoch < best_mae:\n",
        "            best_mae = mae_val_epoch\n",
        "            best_epoch = epoch+1\n",
        "\n",
        "            df = pd.DataFrame()\n",
        "            df['preds'] = np.concatenate(preds_).reshape(-1)\n",
        "            df['targets'] = np.concatenate(targets_).reshape(-1)\n",
        "            df.to_csv(os.path.join(CHECKPOINT_PATH, 'preds_test.csv'))\n",
        "\n",
        "            torch.save(sit_model.state_dict(), os.path.join(CHECKPOINT_PATH,'checkpoint.pth'))\n",
        "\n",
        "\n",
        "print('Final results: best model obtained at epoch {} - mean absolute error in weeks {}'.format(best_epoch,best_mae))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqJofxo-xqje"
      },
      "source": [
        "# END OF THE TUTORIAL"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
